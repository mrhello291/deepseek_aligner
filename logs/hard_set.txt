(EngineCore_DP0 pid=3009064) ERROR 11-23 14:37:24 [core.py:842] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(EngineCore_DP0 pid=3009064) Process EngineCore_DP0:
(EngineCore_DP0 pid=3009064) Traceback (most recent call last):
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(EngineCore_DP0 pid=3009064)     sampler_output = self.sampler(
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3009064)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3009064)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 93, in forward
(EngineCore_DP0 pid=3009064)     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 184, in sample
(EngineCore_DP0 pid=3009064)     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3009064)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3009064)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(EngineCore_DP0 pid=3009064)     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(EngineCore_DP0 pid=3009064)     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=3009064) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 406.88 MiB is free. Including non-PyTorch memory, this process has 21.69 GiB memory in use. Of the allocated memory 20.92 G
iB is allocated by PyTorch, with 81.88 MiB allocated in private pools (e.g., CUDA Graphs), and 56.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.
See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=3009064)
(EngineCore_DP0 pid=3009064) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3009064)
(EngineCore_DP0 pid=3009064) Traceback (most recent call last):
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3009064)     self.run()
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3009064)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
(EngineCore_DP0 pid=3009064)     raise e
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=3009064)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=3009064)     super().__init__(
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3009064)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
(EngineCore_DP0 pid=3009064)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=3009064)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3009064)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3009064)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 507, in compile_or_warm_up_model
(EngineCore_DP0 pid=3009064)     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3009064)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3009064)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(EngineCore_DP0 pid=3009064)     raise RuntimeError(
(EngineCore_DP0 pid=3009064) RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 220, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 178, in main
    llm = LLM(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 343, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 174, in from_engine_args
    return cls(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 108, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 640, in __init__
    super().__init__(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 469, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 907, in launch_core_engines
    wait_for_engine_startup(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 964, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 100
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[ALoading datasets...
Total raw samples: 25268
Limiting to 100 samples.
Constructing prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 14:38:44 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 8192, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 14:38:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 14:38:44 [model.py:1745] Using max model len 8192
INFO 11-23 14:38:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3009813) INFO 11-23 14:38:45 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400
, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=3009813) INFO 11-23 14:38:46 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:45729 backend=nccl
[W1123 14:38:56.597068428 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3009813) INFO 11-23 14:38:56 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3009813) INFO 11-23 14:38:56 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3009813) INFO 11-23 14:38:57 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3009813) INFO 11-23 14:38:57 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.41it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.06s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]
(EngineCore_DP0 pid=3009813)
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:01 [default_loader.py:314] Loading weights took 3.44 seconds
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:01 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.913570 seconds
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:08 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/dd7669e6fa/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:08 [backends.py:647] Dynamo bytecode transform time: 6.03 s
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:11 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.965 s
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:12 [monitor.py:34] torch.compile takes 8.99 s in total
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:15 [gpu_worker.py:359] Available KV cache memory: 4.95 GiB
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:15 [kv_cache_utils.py:1229] GPU KV cache size: 40,560 tokens
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:15 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 4.95x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:03<00:00, 13.96it/s]
Capturing CUDA graphs (decode, FULL):  49%|█████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                      | 17/35 [00:01<00:01, 15.32it/s]
Capturing CUDA graphs (decode, FULL):  57%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                     | 20/35 [00:01<00:00, 17.27it/s]
Capturing CUDA graphs (decode, FULL):  74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                   | 26/35 [00:01<00:00, 19.84it/s]
Capturing CUDA graphs (decode, FULL):  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 29/35 [00:01<00:00, 21.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 17.25it/s]
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:21 [gpu_model_runner.py:4244] Graph capturing finished in 6 secs, took 0.56 GiB
(EngineCore_DP0 pid=3009813) INFO 11-23 14:39:21 [core.py:250] init engine (profile, create kv cache, warmup model) took 20.16 seconds
INFO 11-23 14:39:22 [llm.py:352] Supported tasks: ['generate']
Generating 100 hard fakes...
Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 274.06it/s]
Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:29<00:00,  3.36it/s, est. speed input: 3448.66 toks/s, output: 301.08 toks/s]
Saving to hard_fakes_vllm.jsonl...
Done.
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 100
Loading datasets...
Total raw samples: 25268
Limiting to 100 samples.
Constructing prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 14:44:06 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 8192, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 14:44:06 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 14:44:06 [model.py:1745] Using max model len 8192
INFO 11-23 14:44:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:07 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400
, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:08 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:41219 backend=nccl
[W1123 14:44:18.692036096 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:18 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:18 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:19 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:19 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.13it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.04it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]
(EngineCore_DP0 pid=3011630)
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:23 [default_loader.py:314] Loading weights took 3.44 seconds
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:23 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.982717 seconds
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:30 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/dd7669e6fa/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:30 [backends.py:647] Dynamo bytecode transform time: 5.89 s
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:33 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.250 s
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:35 [monitor.py:34] torch.compile takes 9.14 s in total
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:37 [gpu_worker.py:359] Available KV cache memory: 4.94 GiB
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:37 [kv_cache_utils.py:1229] GPU KV cache size: 40,464 tokens
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:37 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 4.94x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:03<00:00, 13.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:01<00:00, 17.55it/s]
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:44 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.56 GiB
(EngineCore_DP0 pid=3011630) INFO 11-23 14:44:44 [core.py:250] init engine (profile, create kv cache, warmup model) took 20.67 seconds
INFO 11-23 14:44:45 [llm.py:352] Supported tasks: ['generate']
Generating 100 hard fakes...
Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 272.47it/s]
Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30<00:00,  3.23it/s, est. speed input: 3314.96 toks/s, output: 290.15 toks/s]
Saving to hard_fakes_vllm.jsonl...
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 278, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 272, in main
    "type": item["type"]
KeyError: 'type'
ERROR 11-23 14:45:17 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 100
Loading datasets...
Total raw samples: 25268
Limiting to 100 samples.
Constructing prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 14:50:01 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 8192, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 14:50:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 14:50:01 [model.py:1745] Using max model len 8192
INFO 11-23 14:50:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:02 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400
, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:03 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:35473 backend=nccl
[W1123 14:50:13.503044907 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:13 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:14 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.15it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.12it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.11it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.42it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]
(EngineCore_DP0 pid=3013570)
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:17 [default_loader.py:314] Loading weights took 3.15 seconds
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:18 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.688420 seconds
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:24 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/dd7669e6fa/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:24 [backends.py:647] Dynamo bytecode transform time: 6.25 s
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:28 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.364 s
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:30 [monitor.py:34] torch.compile takes 9.61 s in total
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:32 [gpu_worker.py:359] Available KV cache memory: 4.94 GiB
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:32 [kv_cache_utils.py:1229] GPU KV cache size: 40,416 tokens
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:32 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 4.93x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:04<00:00, 12.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 17.16it/s]
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:39 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.54 GiB
(EngineCore_DP0 pid=3013570) INFO 11-23 14:50:39 [core.py:250] init engine (profile, create kv cache, warmup model) took 21.08 seconds
INFO 11-23 14:50:40 [llm.py:352] Supported tasks: ['generate']
Generating 100 hard fakes...
Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 356.85it/s]
Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.20it/s, est. speed input: 3291.73 toks/s, output: 281.45 toks/s]
Saving to hard_fakes_vllm.jsonl...
Successfully saved 100 / 100 items.
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 100
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 204
    temperature=0.6
                ^^^
SyntaxError: invalid syntax. Perhaps you forgot a comma?
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 100
Loading datasets...
Total raw samples: 25268
Limiting to 100 samples.
Constructing prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 15:03:41 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 8192, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 15:03:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 15:03:41 [model.py:1745] Using max model len 8192
INFO 11-23 15:03:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:42 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400
, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:37053 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:48 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:49 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:50 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:50 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.41it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.16it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.13it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.44it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.34it/s]
(EngineCore_DP0 pid=3018187)
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:53 [default_loader.py:314] Loading weights took 3.05 seconds
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:53 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.545789 seconds
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:59 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/dd7669e6fa/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3018187) INFO 11-23 15:03:59 [backends.py:647] Dynamo bytecode transform time: 5.79 s
(EngineCore_DP0 pid=3018187) INFO 11-23 15:04:03 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.147 s
(EngineCore_DP0 pid=3018187) INFO 11-23 15:04:04 [monitor.py:34] torch.compile takes 8.94 s in total
(EngineCore_DP0 pid=3018187) INFO 11-23 15:04:06 [gpu_worker.py:359] Available KV cache memory: 4.95 GiB
(EngineCore_DP0 pid=3018187) INFO 11-23 15:04:07 [kv_cache_utils.py:1229] GPU KV cache size: 40,560 tokens
(EngineCore_DP0 pid=3018187) INFO 11-23 15:04:07 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 4.95x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:03<00:00, 12.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 15.94it/s]
(EngineCore_DP0 pid=3018187) INFO 11-23 15:04:14 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.56 GiB
(EngineCore_DP0 pid=3018187) INFO 11-23 15:04:14 [core.py:250] init engine (profile, create kv cache, warmup model) took 20.40 seconds
INFO 11-23 15:04:15 [llm.py:352] Supported tasks: ['generate']
Generating 100 hard fakes...
Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 258.60it/s]
Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30<00:00,  3.27it/s, est. speed input: 3354.12 toks/s, output: 292.21 toks/s]
Saving to hard_fakes_vllm.jsonl...
Successfully saved 100 / 100 items.
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
Loading datasets...
Total raw samples: 25268
Limiting to 15000 samples.
Constructing prompts...
Skipping index 498 (Length: 31813)
Skipping index 555 (Length: 58743)
Skipping index 1119 (Length: 27316)
Skipping index 1265 (Length: 25463)
Skipping index 1383 (Length: 31597)
Skipping index 1464 (Length: 50193)
Skipping index 1788 (Length: 57111)
Skipping index 2041 (Length: 50754)
Skipping index 2537 (Length: 66675)
Skipping index 3245 (Length: 37458)
Skipping index 3504 (Length: 29852)
Skipping index 3870 (Length: 30210)
Skipping index 4181 (Length: 31245)
Skipping index 4563 (Length: 44389)
Skipping index 4758 (Length: 39782)
Skipping index 5004 (Length: 32151)
Skipping index 5113 (Length: 66789)
Skipping index 5130 (Length: 33341)
Skipping index 5490 (Length: 71055)
Skipping index 5776 (Length: 43558)
Skipping index 5991 (Length: 29796)
Skipping index 6093 (Length: 63847)
Skipping index 6263 (Length: 37643)
Skipping index 6276 (Length: 79209)
Skipping index 6538 (Length: 45174)
Skipping index 6641 (Length: 25262)
Skipping index 6676 (Length: 36083)
Skipping index 6852 (Length: 46501)
Skipping index 7429 (Length: 26154)
Skipping index 7446 (Length: 40517)
Skipping index 7610 (Length: 31544)
Skipping index 7734 (Length: 29421)
Skipping index 7974 (Length: 63710)
Skipping index 7989 (Length: 66825)
Skipping index 8492 (Length: 48168)
Skipping index 9065 (Length: 27910)
Skipping index 9704 (Length: 50221)
Skipping index 10110 (Length: 49368)
Skipping index 10294 (Length: 28230)
Skipping index 10314 (Length: 72186)
Skipping index 10690 (Length: 92487)
Skipping index 10814 (Length: 29645)
Skipping index 10864 (Length: 31565)
Skipping index 11243 (Length: 33329)
Skipping index 11461 (Length: 51787)
Skipping index 11903 (Length: 30418)
Skipping index 12077 (Length: 25958)
Skipping index 12353 (Length: 25158)
Skipping index 12491 (Length: 46074)
Skipping index 12662 (Length: 42494)
Skipping index 12734 (Length: 28655)
Skipping index 12875 (Length: 33679)
Skipping index 13120 (Length: 31300)
Skipping index 13183 (Length: 47921)
Skipping index 13759 (Length: 45230)
Skipping index 14063 (Length: 29492)
Skipping index 14130 (Length: 27274)
Skipping index 14451 (Length: 97419)
Skipping index 14457 (Length: 33193)
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 15:36:31 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 8192, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 15:36:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 15:36:31 [model.py:1745] Using max model len 8192
INFO 11-23 15:36:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:32 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400
, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:41465 backend=nccl
[W1123 15:36:43.349046264 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:43 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:44 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:44 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.39it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.12it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.22it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]
(EngineCore_DP0 pid=3028414)
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:48 [default_loader.py:314] Loading weights took 3.52 seconds
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:48 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory a
nd 4.233880 seconds
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:54 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/dd7669e6fa/rank_0_0/backbone for vLLM's
 torch.compile
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:54 [backends.py:647] Dynamo bytecode transform time: 5.77 s
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:58 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.949 s
(EngineCore_DP0 pid=3028414) INFO 11-23 15:36:59 [monitor.py:34] torch.compile takes 8.72 s in total
(EngineCore_DP0 pid=3028414) INFO 11-23 15:37:01 [gpu_worker.py:359] Available KV cache memory: 4.95 GiB
(EngineCore_DP0 pid=3028414) INFO 11-23 15:37:02 [kv_cache_utils.py:1229] GPU KV cache size: 40,560 tokens
(EngineCore_DP0 pid=3028414) INFO 11-23 15:37:02 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 4.95x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:03<00:00, 14.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:01<00:00, 17.85it/s]
(EngineCore_DP0 pid=3028414) INFO 11-23 15:37:08 [gpu_model_runner.py:4244] Graph capturing finished in 6 secs, took 0.56 GiB
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842] EngineCore failed to start.
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842] Traceback (most recent call last):
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     sampler_output = self.sampler(
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 93, in forward
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 184, in sample
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 425.38 MiB is free. Including non-PyTorch memory, this process has 21.69 GiB memory in u
se. Of the allocated memory 20.91 GiB is allocated by PyTorch, with 81.88 MiB allocated in private pools (e.g., CUDA Graphs), and 72.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segm
ents:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842] Traceback (most recent call last):
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     super().__init__(
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 507, in compile_or_warm_up_model
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842]     raise RuntimeError(
(EngineCore_DP0 pid=3028414) ERROR 11-23 15:37:08 [core.py:842] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(EngineCore_DP0 pid=3028414) Process EngineCore_DP0:
(EngineCore_DP0 pid=3028414) Traceback (most recent call last):
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(EngineCore_DP0 pid=3028414)     sampler_output = self.sampler(
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3028414)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3028414)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 93, in forward
(EngineCore_DP0 pid=3028414)     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 184, in sample
(EngineCore_DP0 pid=3028414)     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3028414)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3028414)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(EngineCore_DP0 pid=3028414)     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(EngineCore_DP0 pid=3028414)     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=3028414) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 425.38 MiB is free. Including non-PyTorch memory, this process has 21.69 GiB memory in use. Of the allocated memory 20.91 G
iB is allocated by PyTorch, with 81.88 MiB allocated in private pools (e.g., CUDA Graphs), and 72.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.
See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=3028414)
(EngineCore_DP0 pid=3028414) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3028414)
(EngineCore_DP0 pid=3028414) Traceback (most recent call last):
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3028414)     self.run()
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3028414)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
(EngineCore_DP0 pid=3028414)     raise e
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=3028414)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=3028414)     super().__init__(
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3028414)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
(EngineCore_DP0 pid=3028414)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=3028414)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3028414)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3028414)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 507, in compile_or_warm_up_model
(EngineCore_DP0 pid=3028414)     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3028414)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3028414)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(EngineCore_DP0 pid=3028414)     raise RuntimeError(
(EngineCore_DP0 pid=3028414) RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 321, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 193, in main
    llm = LLM(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 343, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 174, in from_engine_args
    return cls(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 108, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 640, in __init__
    super().__init__(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 469, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 907, in launch_core_engines
    wait_for_engine_startup(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 964, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
Loading datasets...
Total raw samples: 25268
Limiting to 15000 samples.
Constructing prompts...
Skipping index 498 (Length: 31813)
Skipping index 555 (Length: 58743)
Skipping index 1119 (Length: 27316)
Skipping index 1265 (Length: 25463)
Skipping index 1383 (Length: 31597)
Skipping index 1464 (Length: 50193)
Skipping index 1788 (Length: 57111)
Skipping index 2041 (Length: 50754)
Skipping index 2537 (Length: 66675)
Skipping index 3245 (Length: 37458)
Skipping index 3504 (Length: 29852)
Skipping index 3870 (Length: 30210)
Skipping index 4181 (Length: 31245)
Skipping index 4563 (Length: 44389)
Skipping index 4758 (Length: 39782)
Skipping index 5004 (Length: 32151)
Skipping index 5113 (Length: 66789)
Skipping index 5130 (Length: 33341)
Skipping index 5490 (Length: 71055)
Skipping index 5776 (Length: 43558)
Skipping index 5991 (Length: 29796)
Skipping index 6093 (Length: 63847)
Skipping index 6263 (Length: 37643)
Skipping index 6276 (Length: 79209)
Skipping index 6538 (Length: 45174)
Skipping index 6641 (Length: 25262)
Skipping index 6676 (Length: 36083)
Skipping index 6852 (Length: 46501)
Skipping index 7429 (Length: 26154)
Skipping index 7446 (Length: 40517)
Skipping index 7610 (Length: 31544)
Skipping index 7734 (Length: 29421)
Skipping index 7974 (Length: 63710)
Skipping index 7989 (Length: 66825)
Skipping index 8492 (Length: 48168)
Skipping index 9065 (Length: 27910)
Skipping index 9704 (Length: 50221)
Skipping index 10110 (Length: 49368)
Skipping index 10294 (Length: 28230)
Skipping index 10314 (Length: 72186)
Skipping index 10690 (Length: 92487)
Skipping index 10814 (Length: 29645)
Skipping index 10864 (Length: 31565)
Skipping index 11243 (Length: 33329)
Skipping index 11461 (Length: 51787)
Skipping index 11903 (Length: 30418)
Skipping index 12077 (Length: 25958)
Skipping index 12353 (Length: 25158)
Skipping index 12491 (Length: 46074)
Skipping index 12662 (Length: 42494)
Skipping index 12734 (Length: 28655)
Skipping index 12875 (Length: 33679)
Skipping index 13120 (Length: 31300)
Skipping index 13183 (Length: 47921)
Skipping index 13759 (Length: 45230)
Skipping index 14063 (Length: 29492)
Skipping index 14130 (Length: 27274)
Skipping index 14451 (Length: 97419)
Skipping index 14457 (Length: 33193)
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 15:37:37 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 8192, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 15:37:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 15:37:37 [model.py:1745] Using max model len 8192
INFO 11-23 15:37:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:38 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400
, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:39 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:56141 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:44 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:45 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:46 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:46 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.30it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.03it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.20it/s]
(EngineCore_DP0 pid=3028885)
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:49 [default_loader.py:314] Loading weights took 3.37 seconds
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:50 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.776580 seconds
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:56 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/dd7669e6fa/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:56 [backends.py:647] Dynamo bytecode transform time: 5.90 s
(EngineCore_DP0 pid=3028885) INFO 11-23 15:37:59 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.909 s
(EngineCore_DP0 pid=3028885) INFO 11-23 15:38:01 [monitor.py:34] torch.compile takes 8.81 s in total
(EngineCore_DP0 pid=3028885) INFO 11-23 15:38:03 [gpu_worker.py:359] Available KV cache memory: 4.95 GiB
(EngineCore_DP0 pid=3028885) INFO 11-23 15:38:03 [kv_cache_utils.py:1229] GPU KV cache size: 40,560 tokens
(EngineCore_DP0 pid=3028885) INFO 11-23 15:38:03 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 4.95x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:03<00:00, 13.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 17.28it/s]
(EngineCore_DP0 pid=3028885) INFO 11-23 15:38:10 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.56 GiB
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842] EngineCore failed to start.
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842] Traceback (most recent call last):
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     sampler_output = self.sampler(
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 93, in forward
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 184, in sample
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 425.38 MiB is free. Including non-PyTorch memory, this process has 21.69 GiB memory in u
se. Of the allocated memory 20.91 GiB is allocated by PyTorch, with 81.88 MiB allocated in private pools (e.g., CUDA Graphs), and 72.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segm
ents:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842] Traceback (most recent call last):
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     super().__init__(
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 507, in compile_or_warm_up_model
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842]     raise RuntimeError(
(EngineCore_DP0 pid=3028885) ERROR 11-23 15:38:10 [core.py:842] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(EngineCore_DP0 pid=3028885) Process EngineCore_DP0:
(EngineCore_DP0 pid=3028885) Traceback (most recent call last):
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(EngineCore_DP0 pid=3028885)     sampler_output = self.sampler(
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3028885)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3028885)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 93, in forward
(EngineCore_DP0 pid=3028885)     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 184, in sample
(EngineCore_DP0 pid=3028885)     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3028885)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3028885)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(EngineCore_DP0 pid=3028885)     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(EngineCore_DP0 pid=3028885)     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=3028885) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 425.38 MiB is free. Including non-PyTorch memory, this process has 21.69 GiB memory in use. Of the allocated memory 20.91 G
iB is allocated by PyTorch, with 81.88 MiB allocated in private pools (e.g., CUDA Graphs), and 72.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.
See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=3028885)
(EngineCore_DP0 pid=3028885) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=3028885)
(EngineCore_DP0 pid=3028885) Traceback (most recent call last):
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3028885)     self.run()
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3028885)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
(EngineCore_DP0 pid=3028885)     raise e
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=3028885)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=3028885)     super().__init__(
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=3028885)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 247, in _initialize_kv_caches
(EngineCore_DP0 pid=3028885)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=3028885)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=3028885)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3028885)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 507, in compile_or_warm_up_model
(EngineCore_DP0 pid=3028885)     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3028885)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3028885)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(EngineCore_DP0 pid=3028885)     raise RuntimeError(
(EngineCore_DP0 pid=3028885) RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 321, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 193, in main
    llm = LLM(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 343, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 174, in from_engine_args
    return cls(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 108, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 640, in __init__
    super().__init__(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 469, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 907, in launch_core_engines
    wait_for_engine_startup(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 964, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
Loading datasets...
Total raw samples: 25268
Limiting to 15000 samples.
Constructing prompts...
Skipping index 498 (Length: 31813)
Skipping index 555 (Length: 58743)
Skipping index 1119 (Length: 27316)
Skipping index 1265 (Length: 25463)
Skipping index 1383 (Length: 31597)
Skipping index 1464 (Length: 50193)
Skipping index 1788 (Length: 57111)
Skipping index 2041 (Length: 50754)
Skipping index 2537 (Length: 66675)
Skipping index 3245 (Length: 37458)
Skipping index 3504 (Length: 29852)
Skipping index 3870 (Length: 30210)
Skipping index 4181 (Length: 31245)
Skipping index 4563 (Length: 44389)
Skipping index 4758 (Length: 39782)
Skipping index 5004 (Length: 32151)
Skipping index 5113 (Length: 66789)
Skipping index 5130 (Length: 33341)
Skipping index 5490 (Length: 71055)
Skipping index 5776 (Length: 43558)
Skipping index 5991 (Length: 29796)
Skipping index 6093 (Length: 63847)
Skipping index 6263 (Length: 37643)
Skipping index 6276 (Length: 79209)
Skipping index 6538 (Length: 45174)
Skipping index 6641 (Length: 25262)
Skipping index 6676 (Length: 36083)
Skipping index 6852 (Length: 46501)
Skipping index 7429 (Length: 26154)
Skipping index 7446 (Length: 40517)
Skipping index 7610 (Length: 31544)
Skipping index 7734 (Length: 29421)
Skipping index 7974 (Length: 63710)
Skipping index 7989 (Length: 66825)
Skipping index 8492 (Length: 48168)
Skipping index 9065 (Length: 27910)
Skipping index 9704 (Length: 50221)
Skipping index 10110 (Length: 49368)
Skipping index 10294 (Length: 28230)
Skipping index 10314 (Length: 72186)
Skipping index 10690 (Length: 92487)
Skipping index 10814 (Length: 29645)
Skipping index 10864 (Length: 31565)
Skipping index 11243 (Length: 33329)
Skipping index 11461 (Length: 51787)
Skipping index 11903 (Length: 30418)
Skipping index 12077 (Length: 25958)
Skipping index 12353 (Length: 25158)
Skipping index 12491 (Length: 46074)
Skipping index 12662 (Length: 42494)
Skipping index 12734 (Length: 28655)
Skipping index 12875 (Length: 33679)
Skipping index 13120 (Length: 31300)
Skipping index 13183 (Length: 47921)
Skipping index 13759 (Length: 45230)
Skipping index 14063 (Length: 29492)
Skipping index 14130 (Length: 27274)
Skipping index 14451 (Length: 97419)
Skipping index 14457 (Length: 33193)
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 15:44:52 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 8192, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 15:44:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 15:44:52 [model.py:1745] Using max model len 8192
INFO 11-23 15:44:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 11-23 15:44:52 [vllm.py:500] Cudagraph is disabled under eager mode
(EngineCore_DP0 pid=3031421) INFO 11-23 15:44:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Tru
e, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False
), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, co
mpilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compi
le_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagra
ph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}
(EngineCore_DP0 pid=3031421) INFO 11-23 15:44:54 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:46491 backend=nccl
[W1123 15:45:04.285046678 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:05 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:05 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:06 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:06 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.39it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.14it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.10it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.39it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]
(EngineCore_DP0 pid=3031421)
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:09 [default_loader.py:314] Loading weights took 3.14 seconds
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:09 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.577569 seconds
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:12 [gpu_worker.py:359] Available KV cache memory: 5.34 GiB
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:12 [kv_cache_utils.py:1229] GPU KV cache size: 43,712 tokens
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:12 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 5.34x
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:12 [core.py:250] init engine (profile, create kv cache, warmup model) took 2.81 seconds
(EngineCore_DP0 pid=3031421) INFO 11-23 15:45:13 [vllm.py:500] Cudagraph is disabled under eager mode
INFO 11-23 15:45:13 [llm.py:352] Supported tasks: ['generate']
Generating 14941 hard fakes...
Adding requests:   6%|████████████▋                                                                                                                                                                                                           | 877/14941 [00:02<00:46, 303.17it/s]
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 323, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 214, in main
    outputs = llm.generate(prompts, sampling_params)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 440, in generate
    self._validate_and_add_requests(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1613, in _validate_and_add_requests
    raise e
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1601, in _validate_and_add_requests
    request_id = self._add_request(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1700, in _add_request
    engine_request, tokenization_kwargs = self._process_inputs(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1680, in _process_inputs
    engine_request = self.processor.process_inputs(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 442, in process_inputs
    self._validate_model_inputs(encoder_inputs, decoder_inputs)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 517, in _validate_model_inputs
    self._validate_model_input(decoder_inputs, prompt_type="decoder")
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 591, in _validate_model_input
    raise ValueError(
ValueError: The decoder prompt (length 13832) is longer than the maximum model length of 8192. Make sure that `max_model_len` is no smaller than the number of text tokens.
ERROR 11-23 15:45:17 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
Loading datasets...
Total raw samples: 25268
Limiting to 15000 samples.
Constructing raw prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 15:57:20 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 4096, 'gpu_memory_utilization': 0.95, 'max_num_seqs': 128, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 15:57:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 15:57:20 [model.py:1745] Using max model len 4096
INFO 11-23 15:57:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3035344) INFO 11-23 15:57:21 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph
_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 256, 'local_cache_dir': None}
(EngineCore_DP0 pid=3035344) INFO 11-23 15:57:22 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:55233 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3035344) INFO 11-23 15:57:22 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842] EngineCore failed to start.
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842] Traceback (most recent call last):
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]     super().__init__(
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]     self._init_executor()
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]     self.driver_worker.init_device()
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842]     raise ValueError(
(EngineCore_DP0 pid=3035344) ERROR 11-23 15:57:22 [core.py:842] ValueError: Free memory on device (21.81/23.57 GiB) on startup is less than desired GPU memory utilization (0.95, 22.39 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
(EngineCore_DP0 pid=3035344) Process EngineCore_DP0:
(EngineCore_DP0 pid=3035344) Traceback (most recent call last):
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3035344)     self.run()
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3035344)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
(EngineCore_DP0 pid=3035344)     raise e
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=3035344)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=3035344)     super().__init__(
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=3035344)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=3035344)     self._init_executor()
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=3035344)     self.driver_worker.init_device()
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
(EngineCore_DP0 pid=3035344)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=3035344)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=3035344)     raise ValueError(
(EngineCore_DP0 pid=3035344) ValueError: Free memory on device (21.81/23.57 GiB) on startup is less than desired GPU memory utilization (0.95, 22.39 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 352, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 193, in main
    llm = LLM(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 343, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 174, in from_engine_args
    return cls(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 108, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 640, in __init__
    super().__init__(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 469, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 907, in launch_core_engines
    wait_for_engine_startup(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 964, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[BLoading datasets...
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[BTotal raw samples: 25268
Limiting to 15000 samples.
Constructing raw prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 15:57:49 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 4096, 'max_num_seqs': 128, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 15:57:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 15:57:49 [model.py:1745] Using max model len 4096
INFO 11-23 15:57:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3035585) INFO 11-23 15:57:50 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph
_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 256, 'local_cache_dir': None}
(EngineCore_DP0 pid=3035585) INFO 11-23 15:57:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:35781 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3035585) INFO 11-23 15:57:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3035585) INFO 11-23 15:57:52 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3035585) INFO 11-23 15:57:52 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3035585) INFO 11-23 15:57:52 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.46it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.09it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.36it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]
(EngineCore_DP0 pid=3035585)
(EngineCore_DP0 pid=3035585) INFO 11-23 15:57:56 [default_loader.py:314] Loading weights took 3.13 seconds
(EngineCore_DP0 pid=3035585) INFO 11-23 15:57:56 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.695879 seconds
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:02 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/72a58ebe12/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:02 [backends.py:647] Dynamo bytecode transform time: 5.77 s
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:05 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.796 s
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:06 [monitor.py:34] torch.compile takes 8.57 s in total
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:09 [gpu_worker.py:359] Available KV cache memory: 5.34 GiB
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:09 [kv_cache_utils.py:1229] GPU KV cache size: 43,712 tokens
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:09 [kv_cache_utils.py:1234] Maximum concurrency for 4,096 tokens per request: 10.67x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 15.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:01<00:00, 18.83it/s]
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:13 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took 0.37 GiB
(EngineCore_DP0 pid=3035585) INFO 11-23 15:58:13 [core.py:250] init engine (profile, create kv cache, warmup model) took 17.21 seconds
INFO 11-23 15:58:14 [llm.py:352] Supported tasks: ['generate']
Filtering prompts by exact token count...
Skipped 281 prompts exceeding 3000 tokens.
Proceeding with 14719 prompts.
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 352, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 242, in main
    print(f"Generating {len(prompts)} hard fakes...")
NameError: name 'prompts' is not defined. Did you mean: 'prompt'?
ERROR 11-23 15:58:57 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
Loading datasets...
Total raw samples: 25268
Limiting to 15000 samples.
Constructing raw prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 16:00:07 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 4096, 'max_num_seqs': 128, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 16:00:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 16:00:07 [model.py:1745] Using max model len 4096
INFO 11-23 16:00:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:08 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph
_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 256, 'local_cache_dir': None}
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:09 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:39925 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:09 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:10 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:11 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:11 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.15it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.45it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.36it/s]
(EngineCore_DP0 pid=3036450)
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:14 [default_loader.py:314] Loading weights took 2.99 seconds
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:14 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.611427 seconds
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:20 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/72a58ebe12/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:20 [backends.py:647] Dynamo bytecode transform time: 5.81 s
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:24 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.952 s
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:25 [monitor.py:34] torch.compile takes 8.76 s in total
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:27 [gpu_worker.py:359] Available KV cache memory: 5.34 GiB
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:28 [kv_cache_utils.py:1229] GPU KV cache size: 43,712 tokens
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:28 [kv_cache_utils.py:1234] Maximum concurrency for 4,096 tokens per request: 10.67x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 15.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 22.61it/s]
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:31 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took 0.37 GiB
(EngineCore_DP0 pid=3036450) INFO 11-23 16:00:31 [core.py:250] init engine (profile, create kv cache, warmup model) took 16.95 seconds
INFO 11-23 16:00:32 [llm.py:352] Supported tasks: ['generate']
Filtering prompts by exact token count...
Skipped 281 prompts exceeding 3000 tokens.
Proceeding with 14719 prompts.
Generating 14719 hard fakes...
Adding requests:   1%|█▉                                                                                                                                                                                                                      | 132/14719 [00:00<00:44, 327.39it/s]
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode
=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, en
force_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in
_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler
_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '/home/rs_students/.cache/vllm/torch_compile_cache/72a58ebe12', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['non
e'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mix
er', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel':
True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208
, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 256, 'local_cache_dir': '/home/rs_students/.cache/vllm/torch_compile_cache/72a58ebe12/r
ank_0_0/backbone'},
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [dump_input.py:79] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=1,prompt_token_ids_len=1149,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, f
requency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, pro
mpt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,
 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164],),num_computed_tokens=96,lora_request=N
one,prompt_embeds_shape=None), NewRequestData(req_id=2,prompt_token_ids_len=1411,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_i
d|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured
_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207
, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None), New
RequestData(req_id=3,prompt_token_ids_len=1124,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128
001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),bl
ock_ids=([1, 2, 3, 4, 5, 6, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 29
7, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None), NewRequestData(req_id=4,prompt_token_ids_len=1104,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, freq
uency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt
_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 3
32, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375],),num_computed_tokens=96,lora_request=None,prompt_embeds
_shape=None), NewRequestData(req_id=5,prompt_token_ids_len=1114,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token
_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, ex
tra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,
422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None), NewRequestData(req_id=6,prompt_token_ids_len=2353,mm_features=[],sampling_params=SamplingParams(n=1, presence_penal
ty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=
None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456,
 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511,
 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566,
 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None), NewRequestData(req_id=7,prompt_token_ids_len=1184,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, freque
ncy_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_l
ogprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601
, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None)], scheduled_cached_reqs=CachedRequestData(req_ids=['0'], resumed_req_ids=[], new_token_ids=[], all_token_ids={}, new_block_ids=[null], num_compu
ted_tokens=[1567], num_output_tokens=[1]), num_scheduled_tokens={4: 1008, 6: 2257, 1: 1053, 5: 1018, 0: 1, 2: 1315, 3: 1028, 7: 512}, total_num_scheduled_tokens=8192, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[6], finished_req_ids
=[], free_encoder_mm_hashes=[], pending_structured_output_tokens=false, kv_connector_metadata=null, ec_connector_metadata=null)
Adding requests:   1%|██▍                                                                                                                                                                                                                     | 165/14719 [00:00<00:44, 326.61it/s]
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844] EngineCore encountered a fatal error.
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844] Traceback (most recent call last):
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 835, in run_engine_core
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     engine_core.run_busy_loop()
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 862, in run_busy_loop
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     self._process_engine_step()
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 891, in _process_engine_step
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     outputs, model_executed = self.step_fn()
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 342, in step
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     model_output = future.result()
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/concurrent/futures/_base.py", line 451, in result
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.__get_result()
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     raise self._exception
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 79, in collective_rpc
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 367, in execute_model
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.worker.execute_model(scheduler_output, *args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 563, in execute_model
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     output = self.model_runner.execute_model(
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2799, in execute_model
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     model_output = self._model_forward(
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2621, in _model_forward
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.model(
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     model_output = self.model(
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 399, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     def forward(
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/caching.py", line 53, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     raise e
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "<eval_with_key>.66", line 216, in forward
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weigh
t_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_la
yers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_
= l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self
_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.compiled_graph_for_general_shape(*args)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     graph_output = inductor_compiled_graph(*args)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     return self.current_callable(inputs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     out = model(new_inputs)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]   File "/tmp/torchinductor_rs_students/eu/ceugge6eggdh5hferxi7vdbwavkjau3m7ab633ftz2254tkro5cg.py", line 906, in call
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844]     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
(EngineCore_DP0 pid=3036450) ERROR 11-23 16:01:15 [core.py:844] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 280.31 MiB is free. Including non-PyTorch memory, this process has 21.79 GiB memory in u
se. Of the allocated memory 20.71 GiB is allocated by PyTorch, with 53.88 MiB allocated in private pools (e.g., CUDA Graphs), and 542.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_seg
ments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Adding requests:   1%|██▍                                                                                                                                                                                                                     | 165/14719 [00:00<00:44, 324.00it/s]
(EngineCore_DP0 pid=3036450) Process EngineCore_DP0:
(EngineCore_DP0 pid=3036450) Traceback (most recent call last):
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3036450)     self.run()
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3036450)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
(EngineCore_DP0 pid=3036450)     raise e
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 835, in run_engine_core
(EngineCore_DP0 pid=3036450)     engine_core.run_busy_loop()
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 862, in run_busy_loop
(EngineCore_DP0 pid=3036450)     self._process_engine_step()
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 891, in _process_engine_step
(EngineCore_DP0 pid=3036450)     outputs, model_executed = self.step_fn()
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 342, in step
(EngineCore_DP0 pid=3036450)     model_output = future.result()
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/concurrent/futures/_base.py", line 451, in result
(EngineCore_DP0 pid=3036450)     return self.__get_result()
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
(EngineCore_DP0 pid=3036450)     raise self._exception
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 79, in collective_rpc
(EngineCore_DP0 pid=3036450)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3036450)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 367, in execute_model
(EngineCore_DP0 pid=3036450)     return self.worker.execute_model(scheduler_output, *args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3036450)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 563, in execute_model
(EngineCore_DP0 pid=3036450)     output = self.model_runner.execute_model(
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3036450)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2799, in execute_model
(EngineCore_DP0 pid=3036450)     model_output = self._model_forward(
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2621, in _model_forward
(EngineCore_DP0 pid=3036450)     return self.model(
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
(EngineCore_DP0 pid=3036450)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3036450)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3036450)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
(EngineCore_DP0 pid=3036450)     model_output = self.model(
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 399, in __call__
(EngineCore_DP0 pid=3036450)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
(EngineCore_DP0 pid=3036450)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
(EngineCore_DP0 pid=3036450)     def forward(
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=3036450)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/caching.py", line 53, in __call__
(EngineCore_DP0 pid=3036450)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=3036450)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=3036450)     raise e
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=3036450)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3036450)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3036450)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "<eval_with_key>.66", line 216, in forward
(EngineCore_DP0 pid=3036450)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layer
s_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_mo
dules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_m
odules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_i
nput_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
(EngineCore_DP0 pid=3036450)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
(EngineCore_DP0 pid=3036450)     return self.compiled_graph_for_general_shape(*args)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
(EngineCore_DP0 pid=3036450)     graph_output = inductor_compiled_graph(*args)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=3036450)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
(EngineCore_DP0 pid=3036450)     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=3036450)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=3036450)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=3036450)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=3036450)     return self.current_callable(inputs)
(EngineCore_DP0 pid=3036450)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=3036450)     out = model(new_inputs)
(EngineCore_DP0 pid=3036450)   File "/tmp/torchinductor_rs_students/eu/ceugge6eggdh5hferxi7vdbwavkjau3m7ab633ftz2254tkro5cg.py", line 906, in call
(EngineCore_DP0 pid=3036450)     buf3 = empty_strided_cuda((s72, 28672), (28672, 1), torch.bfloat16)
(EngineCore_DP0 pid=3036450) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 280.31 MiB is free. Including non-PyTorch memory, this process has 21.79 GiB memory in use. Of the allocated memory 20.71 G
iB is allocated by PyTorch, with 53.88 MiB allocated in private pools (e.g., CUDA Graphs), and 542.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.
 See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 352, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 243, in main
    outputs = llm.generate(final_prompts, sampling_params)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 440, in generate
    self._validate_and_add_requests(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1613, in _validate_and_add_requests
    raise e
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1601, in _validate_and_add_requests
    request_id = self._add_request(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1708, in _add_request
    self.llm_engine.add_request(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 259, in add_request
    self.engine_core.add_request(request)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 742, in add_request
    self._send_input(EngineCoreRequestType.ADD, request)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 715, in _send_input
    self.ensure_alive()
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 558, in ensure_alive
    raise EngineDeadError()
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
Loading datasets...
Total raw samples: 25268
Limiting to 15000 samples.
Constructing raw prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 16:02:18 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 4096, 'max_num_seqs': 64, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 16:02:18 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 16:02:18 [model.py:1745] Using max model len 4096
INFO 11-23 16:02:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:19 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {
}, 'max_cudagraph_capture_size': 128, 'local_cache_dir': None}
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:60363 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:20 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:20 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:21 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:21 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.37it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.17it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.12it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]
(EngineCore_DP0 pid=3037264)
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:24 [default_loader.py:314] Loading weights took 3.27 seconds
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:25 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.699155 seconds
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:31 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/72a58ebe12/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:31 [backends.py:647] Dynamo bytecode transform time: 5.54 s
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:34 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.016 s
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:36 [monitor.py:34] torch.compile takes 8.56 s in total
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:38 [gpu_worker.py:359] Available KV cache memory: 5.34 GiB
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:38 [kv_cache_utils.py:1229] GPU KV cache size: 43,712 tokens
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:38 [kv_cache_utils.py:1234] Maximum concurrency for 4,096 tokens per request: 10.67x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 20.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 23.65it/s]
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:40 [gpu_model_runner.py:4244] Graph capturing finished in 2 secs, took 0.23 GiB
(EngineCore_DP0 pid=3037264) INFO 11-23 16:02:40 [core.py:250] init engine (profile, create kv cache, warmup model) took 15.15 seconds
INFO 11-23 16:02:41 [llm.py:352] Supported tasks: ['generate']
Filtering prompts by exact token count...
Skipped 281 prompts exceeding 3000 tokens.
Proceeding with 14719 prompts.
Generating 14719 hard fakes...
Adding requests:   2%|████▋                                                                                                                                                                                                                   | 316/14719 [00:00<00:41, 347.22it/s]
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode
=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, en
force_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in
_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler
_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '/home/rs_students/.cache/vllm/torch_compile_cache/72a58ebe12', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['non
e'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mix
er', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel':
True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128], 'cudagraph_copy_inputs': False, 'cudagraph_spec
ialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 128, 'local_cache_dir': '/home/rs_students/.cache/vllm/torch_compile_cache/72a58ebe12/rank_0_0/backbone'},
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [dump_input.py:79] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=3,prompt_token_ids_len=1124,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, f
requency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, pro
mpt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266
, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312],),num_computed_tokens=96,lora_request=None,
prompt_embeds_shape=None), NewRequestData(req_id=4,prompt_token_ids_len=1104,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'
], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_out
puts=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 35
6, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None), NewRequestData(req_id=5,prompt_token_ids_len=1114,mm_features=[],sampling_params=SamplingParams(n=1, presen
ce_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, l
ogprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 3
91, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439],),num_computed_tokens=96,lora_r
equest=None,prompt_embeds_shape=None), NewRequestData(req_id=6,prompt_token_ids_len=2353,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[
'<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, st
ructured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,
481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535,
536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581],),num_computed_tokens=96,lora_request=None,pr
ompt_embeds_shape=None), NewRequestData(req_id=7,prompt_token_ids_len=1184,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'],
 stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outpu
ts=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625,
 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None), NewRequestData(req_id=8,prompt_token_ids_len=1176,mm_features=[],sampling_params=Sam
plingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_token
s=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660
, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715
, 716, 717],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None), NewRequestData(req_id=9,prompt_token_ids_len=1202,mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=
0.95, top_k=0, min_p=0.0, seed=None, stop=['<|eot_id|>'], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_t
okens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 75
0, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762],),num_computed_tokens=96,lora_request=None,prompt_embeds_shape=None)], scheduled_cached_reqs=CachedRequestData(req_ids=['0', '1', '2'], resumed_req_ids=[], new_token_ids=[], all_token_ids={}, new_block_ids=[null,
 null, null], num_computed_tokens=[1567, 1149, 1411], num_output_tokens=[1, 1, 1]), num_scheduled_tokens={5: 1018, 6: 2257, 8: 1080, 0: 1, 9: 710, 2: 1, 4: 1008, 7: 1088, 1: 1, 3: 1028}, total_num_scheduled_tokens=8192, scheduled_spec_decode_tokens={}, scheduled_encoder_inpu
ts={}, num_common_prefix_blocks=[6], finished_req_ids=[], free_encoder_mm_hashes=[], pending_structured_output_tokens=false, kv_connector_metadata=null, ec_connector_metadata=null)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844] EngineCore encountered a fatal error.
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844] Traceback (most recent call last):
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 835, in run_engine_core
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     engine_core.run_busy_loop()
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 862, in run_busy_loop
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     self._process_engine_step()
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 891, in _process_engine_step
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     outputs, model_executed = self.step_fn()
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 342, in step
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     model_output = future.result()
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/concurrent/futures/_base.py", line 451, in result
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.__get_result()
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     raise self._exception
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 79, in collective_rpc
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 367, in execute_model
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.worker.execute_model(scheduler_output, *args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 563, in execute_model
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     output = self.model_runner.execute_model(
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return func(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2799, in execute_model
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     model_output = self._model_forward(
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2621, in _model_forward
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.model(
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     model_output = self.model(
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 399, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     def forward(
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/caching.py", line 53, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     raise e
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "<eval_with_key>.66", line 223, in forward
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     submod_6 = self.submod_6(getitem_13, s72, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weig
ht_, getitem_14, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_
layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_13 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weigh
t_ = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = getitem_14 = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = l_
self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.compiled_graph_for_general_shape(*args)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     graph_output = inductor_compiled_graph(*args)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     return self.current_callable(inputs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     out = model(new_inputs)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]   File "/tmp/torchinductor_rs_students/eu/ceugge6eggdh5hferxi7vdbwavkjau3m7ab633ftz2254tkro5cg.py", line 911, in call
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844]     buf4 = empty_strided_cuda((s72, 14336), (14336, 1), torch.bfloat16)
(EngineCore_DP0 pid=3037264) ERROR 11-23 16:03:23 [core.py:844] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 259.31 MiB is free. Including non-PyTorch memory, this process has 21.81 GiB memory in u
se. Of the allocated memory 21.08 GiB is allocated by PyTorch, with 39.88 MiB allocated in private pools (e.g., CUDA Graphs), and 318.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_seg
ments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Adding requests:   2%|████▉                                                                                                                                                                                                                   | 333/14719 [00:00<00:41, 343.63it/s]
(EngineCore_DP0 pid=3037264) Process EngineCore_DP0:
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 352, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 243, in main
    outputs = llm.generate(final_prompts, sampling_params)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 440, in generate
    self._validate_and_add_requests(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1613, in _validate_and_add_requests
    raise e
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1601, in _validate_and_add_requests
    request_id = self._add_request(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1708, in _add_request
    self.llm_engine.add_request(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 259, in add_request
    self.engine_core.add_request(request)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 742, in add_request
    self._send_input(EngineCoreRequestType.ADD, request)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 715, in _send_input
    self.ensure_alive()
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 558, in ensure_alive
    raise EngineDeadError()
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
(EngineCore_DP0 pid=3037264) Traceback (most recent call last):
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=3037264)     self.run()
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=3037264)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
(EngineCore_DP0 pid=3037264)     raise e
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 835, in run_engine_core
(EngineCore_DP0 pid=3037264)     engine_core.run_busy_loop()
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 862, in run_busy_loop
(EngineCore_DP0 pid=3037264)     self._process_engine_step()
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 891, in _process_engine_step
(EngineCore_DP0 pid=3037264)     outputs, model_executed = self.step_fn()
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 342, in step
(EngineCore_DP0 pid=3037264)     model_output = future.result()
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/concurrent/futures/_base.py", line 451, in result
(EngineCore_DP0 pid=3037264)     return self.__get_result()
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
(EngineCore_DP0 pid=3037264)     raise self._exception
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/executor/uniproc_executor.py", line 79, in collective_rpc
(EngineCore_DP0 pid=3037264)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
(EngineCore_DP0 pid=3037264)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 367, in execute_model
(EngineCore_DP0 pid=3037264)     return self.worker.execute_model(scheduler_output, *args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3037264)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 563, in execute_model
(EngineCore_DP0 pid=3037264)     output = self.model_runner.execute_model(
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=3037264)     return func(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2799, in execute_model
(EngineCore_DP0 pid=3037264)     model_output = self._model_forward(
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 2621, in _model_forward
(EngineCore_DP0 pid=3037264)     return self.model(
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
(EngineCore_DP0 pid=3037264)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3037264)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3037264)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 655, in forward
(EngineCore_DP0 pid=3037264)     model_output = self.model(
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/decorators.py", line 399, in __call__
(EngineCore_DP0 pid=3037264)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/wrapper.py", line 152, in __call__
(EngineCore_DP0 pid=3037264)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 430, in forward
(EngineCore_DP0 pid=3037264)     def forward(
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=3037264)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/caching.py", line 53, in __call__
(EngineCore_DP0 pid=3037264)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=3037264)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=3037264)     raise e
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=3037264)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=3037264)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=3037264)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "<eval_with_key>.66", line 223, in forward
(EngineCore_DP0 pid=3037264)     submod_6 = self.submod_6(getitem_13, s72, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, getitem_14, l_self_modules_lay
ers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_
modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_13 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_
2_modules_post_attention_layernorm_parameters_weight_ = getitem_14 = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_3_modul
es_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py", line 126, in __call__
(EngineCore_DP0 pid=3037264)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/piecewise_backend.py", line 99, in __call__
(EngineCore_DP0 pid=3037264)     return self.compiled_graph_for_general_shape(*args)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/compilation/compiler_interface.py", line 268, in compiled_graph_wrapper
(EngineCore_DP0 pid=3037264)     graph_output = inductor_compiled_graph(*args)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=3037264)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/standalone_compile.py", line 184, in <lambda>
(EngineCore_DP0 pid=3037264)     return CompiledArtifact(lambda *args: compiled_fn(list(args)), None)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=3037264)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=3037264)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=3037264)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=3037264)     return self.current_callable(inputs)
(EngineCore_DP0 pid=3037264)   File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=3037264)     out = model(new_inputs)
(EngineCore_DP0 pid=3037264)   File "/tmp/torchinductor_rs_students/eu/ceugge6eggdh5hferxi7vdbwavkjau3m7ab633ftz2254tkro5cg.py", line 911, in call
(EngineCore_DP0 pid=3037264)     buf4 = empty_strided_cuda((s72, 14336), (14336, 1), torch.bfloat16)
(EngineCore_DP0 pid=3037264) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 259.31 MiB is free. Including non-PyTorch memory, this process has 21.81 GiB memory in use. Of the allocated memory 21.08 G
iB is allocated by PyTorch, with 39.88 MiB allocated in private pools (e.g., CUDA Graphs), and 318.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.
 See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
Loading datasets...
Total raw samples: 25268
Limiting to 15000 samples.
Constructing raw prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 16:05:35 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 2048, 'max_num_seqs': 64, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 16:05:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 16:05:35 [model.py:1745] Using max model len 2048
INFO 11-23 16:05:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {
}, 'max_cudagraph_capture_size': 128, 'local_cache_dir': None}
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:57805 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:38 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:38 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:38 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.04it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.04it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]
(EngineCore_DP0 pid=3038425)
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:42 [default_loader.py:314] Loading weights took 3.44 seconds
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:43 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 4.136434 seconds
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:49 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/d328ee0faa/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:49 [backends.py:647] Dynamo bytecode transform time: 6.18 s
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:50 [backends.py:251] Cache the graph for dynamic shape for later use
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:55 [backends.py:282] Compiling a graph for dynamic shape takes 5.09 s
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:56 [monitor.py:34] torch.compile takes 11.27 s in total
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:59 [gpu_worker.py:359] Available KV cache memory: 5.34 GiB
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:59 [kv_cache_utils.py:1229] GPU KV cache size: 43,712 tokens
(EngineCore_DP0 pid=3038425) INFO 11-23 16:05:59 [kv_cache_utils.py:1234] Maximum concurrency for 2,048 tokens per request: 21.34x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 20.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 23.07it/s]
(EngineCore_DP0 pid=3038425) INFO 11-23 16:06:01 [gpu_model_runner.py:4244] Graph capturing finished in 2 secs, took 0.23 GiB
(EngineCore_DP0 pid=3038425) INFO 11-23 16:06:01 [core.py:250] init engine (profile, create kv cache, warmup model) took 18.41 seconds
INFO 11-23 16:06:02 [llm.py:352] Supported tasks: ['generate']
Filtering prompts by exact token count...
Skipped 281 prompts exceeding 3000 tokens.
Proceeding with 14719 prompts.
Generating 14719 hard fakes...
Adding requests:   0%|                                                                                                                                                                                                                          | 6/14719 [00:00<01:35, 153.57it/s]
Traceback (most recent call last):
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 352, in <module>
    main()
  File "/home/rs_students/nlpG4/deepseek_aligner/negative_sampling/hard_fake_gen_vllm.py", line 243, in main
    outputs = llm.generate(final_prompts, sampling_params)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 440, in generate
    self._validate_and_add_requests(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1613, in _validate_and_add_requests
    raise e
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1601, in _validate_and_add_requests
    request_id = self._add_request(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1700, in _add_request
    engine_request, tokenization_kwargs = self._process_inputs(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 1680, in _process_inputs
    engine_request = self.processor.process_inputs(
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 442, in process_inputs
    self._validate_model_inputs(encoder_inputs, decoder_inputs)
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 517, in _validate_model_inputs
    self._validate_model_input(decoder_inputs, prompt_type="decoder")
  File "/home/rs_students/tools/miniconda3/envs/vllm_env/lib/python3.10/site-packages/vllm/v1/engine/processor.py", line 591, in _validate_model_input
    raise ValueError(
ValueError: The decoder prompt (length 2353) is longer than the maximum model length of 2048. Make sure that `max_model_len` is no smaller than the number of text tokens.
ERROR 11-23 16:06:43 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ python negative_sampling/hard_fake_gen_vllm.py --limit 15000
Loading datasets...
Total raw samples: 25268
Limiting to 15000 samples.
Constructing raw prompts...
Initializing vLLM with models/Llama-3.1-8B-Instruct...
INFO 11-23 16:11:19 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 4096, 'max_num_seqs': 32, 'disable_log_stats': True, 'model': 'models/Llama-3.1-8B-Instruct'}
INFO 11-23 16:11:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 11-23 16:11:19 [model.py:1745] Using max model len 4096
INFO 11-23 16:11:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision
=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=Fal
se, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=Fals
e), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, c
ompilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_
with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_
indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIE
CEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 64,
 'local_cache_dir': None}
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.40.105:58881 backend=nccl
[W1123 16:11:31.639044499 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:31 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:32 [gpu_model_runner.py:3259] Starting to load model models/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:32 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:32 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.36it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.18it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.37it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.28it/s]
(EngineCore_DP0 pid=3040271)
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:35 [default_loader.py:314] Loading weights took 3.17 seconds
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:36 [gpu_model_runner.py:3338] Model loading took 14.9889 GiB memory and 3.590098 seconds
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:42 [backends.py:631] Using cache directory: /home/rs_students/.cache/vllm/torch_compile_cache/72a58ebe12/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:42 [backends.py:647] Dynamo bytecode transform time: 5.83 s
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:46 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.365 s
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:47 [monitor.py:34] torch.compile takes 9.20 s in total
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:50 [gpu_worker.py:359] Available KV cache memory: 5.37 GiB
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:50 [kv_cache_utils.py:1229] GPU KV cache size: 43,968 tokens
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:50 [kv_cache_utils.py:1234] Maximum concurrency for 4,096 tokens per request: 10.73x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 21.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 21.16it/s]
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:51 [gpu_model_runner.py:4244] Graph capturing finished in 2 secs, took 0.14 GiB
(EngineCore_DP0 pid=3040271) INFO 11-23 16:11:51 [core.py:250] init engine (profile, create kv cache, warmup model) took 15.47 seconds
INFO 11-23 16:11:52 [llm.py:352] Supported tasks: ['generate']
Filtering prompts by exact token count...
Skipped 281 prompts exceeding 3000 tokens.
Proceeding with 14719 prompts.
Generating 14719 hard fakes...
Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14719/14719 [00:54<00:00, 270.81it/s]
Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14719/14719 [1:21:20<00:00,  3.02it/s, est. speed input: 3686.28 toks/s, output: 271.17 toks/s]
Saving to hard_fakes_vllm.jsonl...
Failed to extract index 8: {
  "chosen_sentence": "The song "I Don't Care I L...
Failed to extract index 215: {
  "chosen_sentence": "The cast of Tyler Perry's ...
Failed to extract index 398: {
  "chosen_sentence": "Robert Wadlow, also known ...
Failed to extract index 487: {
  "chosen_sentence": "Kenneth Joseph "Ken" Marin...
Failed to extract index 1086: {
  "chosen_sentence": "The basketball player Eric...
Failed to extract index 1152: {
  "chosen_sentence": "Boltz was virtually unknow...
Failed to extract index 1271: {
  "chosen_sentence": "The song "Bring Me to Life...
Failed to extract index 1597: {
  "chosen_sentence": "The song "Love on the Brai...
Failed to extract index 1634: {
  "chosen_sentence": "Documentation of Pacific h...
Failed to extract index 1786: Here is the rewritten data in the required format:...
Failed to extract index 2162: {
  "chosen_sentence": "The song "When Will I Be L...
Failed to extract index 2461: {
  "chosen_sentence": "Salty "The Sack" Johnson, ...
Failed to extract index 2505: {
  "chosen_sentence": "The term "Palestine" was f...
Failed to extract index 2507: {
  "chosen_sentence": "The song "I Do n't Wanna L...
Failed to extract index 2660: {
  "chosen_sentence": "The song "A Little Party N...
Failed to extract index 2715: {
  "chosen_sentence": "The country ballad "Over Y...
Failed to extract index 2795: {
  "chosen_sentence": "The narrator of "The Tell-...
Failed to extract index 2829: {
  "chosen_sentence": "In the 1930s, a slang vers...
Failed to extract index 2914: {
  "chosen_sentence": "The game show "To Tell the...
Failed to extract index 2997: {
  "chosen_sentence": "Send in the Clowns" is a s...
Failed to extract index 3252: {
  "chosen_sentence": "Gotye stated that the song...
Failed to extract index 3648: {
  "chosen_sentence": "Snowbird" is a song writte...
Failed to extract index 4097: {
  "chosen_sentence": "The Toronto Maple Leafs ha...
Failed to extract index 4394: {
  "chosen_sentence": "The first episode of Spong...
Failed to extract index 4729: {
  "chosen_sentence": "The song "My Sweet Lord" i...
Failed to extract index 4939: {
  "chosen_sentence": "The song "Winchester Cathe...
Failed to extract index 4968: {
  "chosen_sentence": "The song "We Are the World...
Failed to extract index 5358: {
  "chosen_sentence": "Quincy Jones sang "Mack th...
Failed to extract index 5437: {
  "chosen_sentence": "The song "Chapel of Love" ...
Failed to extract index 5597: {
  "chosen_sentence": "The song "Cotton-Eyed Joe"...
Failed to extract index 5690: {
  "chosen_sentence": "In New York City, the Flor...
Failed to extract index 6297: Here are the outputs for the provided text:

**Raw...
Failed to extract index 6325: {
  "chosen_sentence": "The song "This Land Is You...
Failed to extract index 6425: {
  "chosen_sentence": "Willow Smith released her ...
Failed to extract index 6510: {
  "chosen_sentence": "The song "You Can't Always...
Failed to extract index 6698: {
  "chosen_sentence": "Phylicia Rashad played the...
Failed to extract index 6738: {
  "chosen_sentence": "Nicolette Larson sings wit...
Failed to extract index 6911: {
  "chosen_sentence": "The song "You're No Good" ...
Failed to extract index 7115: {
  "chosen_sentence": "Heinz Field has a seating ...
Failed to extract index 7240: {
  "chosen_sentence": "The quote "If at first you...
Failed to extract index 7308: {
  "chosen_sentence": "Jason Lewis played the rec...
Failed to extract index 7324: {
  "chosen_sentence": "The term "commonwealth" is...
Failed to extract index 7382: {
  "chosen_sentence": "The song "Nobody Knows You...
Failed to extract index 7469: {
  "chosen_sentence": "The fictional town of Nate...
Failed to extract index 7758: {
  "chosen_sentence": "The song "You've Lost That...
Failed to extract index 7905: {
  "chosen_sentence": "The song "I Can't Tell You...
Failed to extract index 7964: {
  "chosen_sentence": "The American television so...
Failed to extract index 8198: {
  "chosen_sentence": "The song "You Can't Take t...
Failed to extract index 8234: {
  "chosen_sentence": "The blues scale and harmon...
Failed to extract index 8369: {
  "chosen_sentence": "Bauhaus covered "Spirit In...
Failed to extract index 8859: {
  "chosen_sentence": "The song "A Teenager in Lo...
Failed to extract index 8936: {
  "chosen_sentence": "The American folk music ba...
Failed to extract index 9074: {
  "chosen_sentence": "Leon "Kida" Burns was the ...
Failed to extract index 9091: {
  "chosen_sentence": "In the episode "Freak the ...
Failed to extract index 9216: {
  "chosen_sentence": "Cinco de Mayo, a celebrati...
Failed to extract index 9307: Here are the rewritten sentences:

**Fact 1:** 477...
Failed to extract index 9396: {
  "chosen_sentence": "Raymond "Ray" Park played ...
Failed to extract index 9405: {
  "chosen_sentence": "The 1983 song "Bang the Dr...
Failed to extract index 9458: {
  "chosen_sentence": "Luke wants to find the gir...
Failed to extract index 9464: {
  "chosen_sentence": "The Torres Strait Islander...
Failed to extract index 9528: {
  "chosen_sentence": "The song "Somebody That I ...
Failed to extract index 9531: Here is the rewritten data in the required format:...
Failed to extract index 10155: {
  "chosen_sentence": "A woman who is 5'6" (169 c...
Failed to extract index 10225: {
  "chosen_sentence": "Madonna played the role of...
Failed to extract index 10452: {
  "chosen_sentence": "The Montgomery Biscuits go...
Failed to extract index 10455: {
  "chosen_sentence": "She provided the female vo...
Failed to extract index 11043: {
  "chosen_sentence": "In the Northern Hemisphere...
Failed to extract index 11114: {
  "chosen_sentence": "Socket wrenches are typica...
Failed to extract index 11310: {
  "chosen_sentence": "The song "The Song That Ne...
Failed to extract index 11561: {
  "chosen_sentence": "Leon "Kida" Burns was the ...
Failed to extract index 11574: {
  "chosen_sentence": "Games that support cross-p...
Failed to extract index 11861: {
  "chosen_sentence": "The Bangladesh team consis...
Failed to extract index 12123: {
  "chosen_sentence": "Alfred Thomas "Freddie" Hi...
Failed to extract index 12251: {
  "chosen_sentence": "The animated TV special "M...
Failed to extract index 12310: {
  "chosen_sentence": "She Said" is the second si...
Failed to extract index 12498: {
  "chosen_sentence": "The song "Love in This Clu...
Failed to extract index 12546: {
  "chosen_sentence": "The song "Drink a Beer" is...
Failed to extract index 12570: {
  "chosen_sentence": "Ed "Too Tall" Jones played...
Failed to extract index 12647: {
  "chosen_sentence": "Edd Byrnes, a young actor,...
Failed to extract index 12779: {
  "chosen_sentence": "You're Sixteen", a song wr...
Failed to extract index 12809: {
  "chosen_sentence": "In the US, graduate diplom...
Failed to extract index 12856: {
  "chosen_sentence": "The phrase "goose bumps" o...
Failed to extract index 12871: {
  "chosen_sentence": "The song "American Pie" by...
Failed to extract index 12889: {
  "chosen_sentence": "The song "La La Means I Lo...
Failed to extract index 13051: {
  "chosen_sentence": "Mariah Carey's single "Do ...
Failed to extract index 13077: {
  "chosen_sentence": "The song "When You Believe...
Failed to extract index 13216: {
  "chosen_sentence": "Navi Rawan Aghdashloo star...
Failed to extract index 13355: {
  "chosen_sentence": "Toni Braxton released the ...
Failed to extract index 13380: {
  "chosen_sentence": "The narrator of "The Tell-...
Failed to extract index 13411: {
  "chosen_sentence": "Numerous men have come for...
Failed to extract index 13801: {
  "chosen_sentence": "The song "(Our Love) Don't...
Failed to extract index 13894: {
  "chosen_sentence": "The song "Owner of a Lonel...
Failed to extract index 14264: {
  "chosen_sentence": "The song "Man! I Feel Like...
Failed to extract index 14363: {
  "chosen_sentence": "The Patriots' "Flying Elvi...
Failed to extract index 14602: {
  "chosen_sentence": "The construction of large ...
Done.
Saved: 14620
Skipped (Context too long to fit 2048): 0
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ tmux capture -S--E-
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ tmux save-buffer ./logs/hard_set.txt
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ tmux delete-buffer
(vllm_env) rs_students@dslab:~/nlpG4/deepseek_aligner$ tmux capture-pane -S - -E - -p > ./logs/hard_set.txt

